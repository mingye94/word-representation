{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lingo-mit/6864-hw1/blob/master/6864_hw1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N655YeL2eEUC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '6864-hw1'...\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
    "# rm -rf 6864-hw1\n",
    "# git clone https://github.com/lingo-mit/6864-hw1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5R8vijdeKgl"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/6864-hw1\")\n",
    "\n",
    "import csv\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "import lab_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WaoYiysseNIH"
   },
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "In the remaining part of the lab (containing part 3) you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. Answers to questions in this lab should go in the same report as the initial release.\n",
    "\n",
    "As before, we'll start by loading up a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUn-q_pIeuAV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
      "rating: 1 (good)\n",
      "\n",
      "Read 4000 total reviews.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "n_positive = 0\n",
    "n_disp = 0\n",
    "with open(\"reviews.csv\") as reader:\n",
    "  csvreader = csv.reader(reader)\n",
    "  next(csvreader)\n",
    "  for id, review, label in csvreader:\n",
    "    label = int(label)\n",
    "\n",
    "    # hacky class balancing\n",
    "    if label == 1:\n",
    "      if n_positive == 2000:\n",
    "        continue\n",
    "      n_positive += 1\n",
    "    if len(data) == 4000:\n",
    "      break\n",
    "\n",
    "    data.append((review, label))\n",
    "    \n",
    "    if n_disp > 5:\n",
    "      continue\n",
    "    n_disp += 1\n",
    "    print(\"review:\", review)\n",
    "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Read {len(data)} total reviews.\")\n",
    "np.random.shuffle(data)\n",
    "reviews, labels = zip(*data)\n",
    "train_reviews = reviews[:3000]\n",
    "train_labels = labels[:3000]\n",
    "val_reviews = reviews[3000:3500]\n",
    "val_labels = labels[3000:3500]\n",
    "test_reviews = reviews[3500:]\n",
    "test_labels = labels[3500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2qlqRHoe3y-"
   },
   "source": [
    "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
    "\n",
    "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll probably want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(a+b) = logaddexp(a, b)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wVf4QVIfBdc"
   },
   "outputs": [],
   "source": [
    "# hmm model\n",
    "class HMM(object):\n",
    "    def __init__(self, num_states, num_words):\n",
    "        self.num_states = num_states\n",
    "        self.num_words = num_words\n",
    "\n",
    "        self.states = range(num_states)\n",
    "        self.symbols = range(num_words)\n",
    "\n",
    "        # initialize the matrix A with random transition probabilities p(j|i)\n",
    "        # A should be a matrix of size `num_states x num_states`\n",
    "        # with rows that sum to 1\n",
    "        # your code here\n",
    "        #self.A = np.random.rand(num_states, num_states)\n",
    "        #for i in range(num_states):\n",
    "            #self.A[i,:]/= self.A[i,:].sum()\n",
    "        #print(self.A.shape)\n",
    "        self.A = np.random.dirichlet(np.ones(num_states), num_states)\n",
    "        # initialize the matrix B with random emission probabilities p(o|i)\n",
    "        # B should be a matrix of size `num_states x num_words`\n",
    "        # with rows that sum to 1\n",
    "        #self.B = np.random.rand(num_states, num_words)\n",
    "        #for i in range(num_states):\n",
    "            #self.B[i,:]/= self.B[i,:].sum()\n",
    "        #print(self.B.shape)\n",
    "        self.B = np.random.dirichlet(np.ones(num_words), num_states)\n",
    "            \n",
    "        # initialize the vector pi with a random starting distribution\n",
    "        # pi should be a vector of size `num_states`\n",
    "        # your code here\n",
    "        #self.pi = np.random.rand(num_states)\n",
    "        #self.pi /= self.pi.sum()\n",
    "        self.pi = np.random.dirichlet(np.ones(num_states), 1).flatten()\n",
    "        #print(self.pi.shape)\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"randomly sample the HMM to generate a sequence.\n",
    "        \"\"\"\n",
    "        # we'll give you this one\n",
    "\n",
    "        sequence = []\n",
    "        # initialize the first state\n",
    "        state = np.random.choice(self.states, p=self.pi)\n",
    "        for i in range(n):\n",
    "            # get the emission probs for this state\n",
    "            b = self.B[state, :]\n",
    "            # emit a word\n",
    "            word = np.random.choice(self.symbols, p=b)\n",
    "            sequence.append(word)\n",
    "            # get the transition probs for this state\n",
    "            a = self.A[state, :]\n",
    "            # update the state\n",
    "            state = np.random.choice(self.states, p=a)\n",
    "        return sequence\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # run the forward algorithm\n",
    "        # this function should return a `len(obs) x num_states` matrix\n",
    "        # where the (i, j)th entry contains p(obs[:t], hidden_state_t = i)\n",
    "        \n",
    "        log_A = np.log(self.A)\n",
    "        log_B = np.log(self.B)\n",
    "        log_pi = np.log(self.pi)\n",
    "        alpha = np.zeros((len(obs), self.num_states))\n",
    "        # your code here!\n",
    "        #alpha[0,:] = self.pi*self.B[:,obs[0]]\n",
    "        #alpha[0,:] = log_pi + log_B[:,obs[0]]\n",
    "        for i in range(self.num_states):\n",
    "            #alpha[0,i] = self.pi[i]*self.B[i,obs[0]]\n",
    "            alpha[0,i] = log_pi[i] + log_B[i, obs[0]]\n",
    "            \n",
    "        for i in range(1, len(obs)):\n",
    "            for j in range(self.num_states):\n",
    "                #sum_alpha_a = alpha[i-1,:].dot(self.A[:,j])\n",
    "                sum_alpha_a = logsumexp(alpha[i-1,:] + log_A[:,j])\n",
    "                #alpha[i,j] = sum_alpha_a*self.B[j,obs[i]]\n",
    "                alpha[i,j] = sum_alpha_a + log_B[j,obs[i]]\n",
    "            #alpha[i,:] = alpha[i,:]/alpha[i,:].sum()\n",
    "        \n",
    "        #alpha += 1e-300\n",
    "        #return np.log(alpha)\n",
    "        return alpha\n",
    "\n",
    "    def backward(self, obs):\n",
    "        # run the backward algorithm\n",
    "        # this function should return a `len(obs) x num_states` matrix\n",
    "        # where the (i, j)th entry contains p(obs[t+1:] | hidden_state_t = i)\n",
    "        \n",
    "        beta = np.log(np.ones((len(obs), self.num_states)))\n",
    "        #beta1 = np.log(np.ones((len(obs), self.num_states))) \n",
    "        log_A = np.log(self.A)\n",
    "        log_B = np.log(self.B)\n",
    "        log_pi = np.log(self.pi)\n",
    "        \n",
    "        # your code here!\n",
    "        \n",
    "        for i in range(len(obs)-2, -1, -1):\n",
    "            for j in range(self.num_states):\n",
    "                #beta[i, j] = (self.A[j,:]*beta[i+1,:]).dot(self.B[:,obs[i+1]])\n",
    "                beta[i,j] = logsumexp(log_A[j,:] + beta[i+1,:] + log_B[:,obs[i+1]])\n",
    "            #beta[i,:] = beta[i,:]/beta[i,:].sum()\n",
    "        \n",
    "        #for i in range(len(obs)-2, -1, -1):\n",
    "            #for j in range(self.num_states):\n",
    "                #beta[i, j] = (self.A[j,:]*beta[i+1,:]).dot(self.B[:,obs[i+1]])\n",
    "            \n",
    "                \n",
    "        #beta += 1e-300\n",
    "        #return np.log(beta)\n",
    "        return beta\n",
    "        \n",
    "    def forward_backward(self, obs):\n",
    "        # compute forward--backward scores\n",
    "\n",
    "        # logprob is the total log-probability of the sequence obs \n",
    "        # (marginalizing over hidden states)\n",
    "\n",
    "        # gamma is a matrix of size `len(obs) x num_states`\n",
    "        # it contains the marginal probability of being in state i at time t\n",
    "\n",
    "        # xi is a tensor of size `len(obs) x num_states x num_states`\n",
    "        # it conains the marginal probability of transitioning from i to j at t\n",
    "\n",
    "        log_A = np.log(self.A)\n",
    "        log_B = np.log(self.B)\n",
    "        log_pi = np.log(self.pi)\n",
    "        \n",
    "        # your code here!\n",
    "        alpha = self.forward(obs)\n",
    "        beta = self.backward(obs)\n",
    "        \n",
    "        #prob_o_lambda = alpha[-1,:].sum()\n",
    "        logprob = logsumexp(alpha[-1,:])\n",
    "        #logprob = logsumexp(log_pi + log_B[:,0].T + beta[0,:])\n",
    "        \n",
    "        xi = np.zeros((len(obs), self.num_states, self.num_states))\n",
    "        gamma = np.zeros((len(obs), self.num_states))\n",
    "        #print(gamma.shape)\n",
    "        \n",
    "        for i in range(len(obs)-1):\n",
    "            for j in range(self.num_states):\n",
    "                #gamma[i,j] = alpha[i,j]*beta[i,j]/sum_gamma\n",
    "                for k in range(self.num_states):\n",
    "                    #xi[i,j,k] = (alpha[i,j]*self.A[j,k]*self.B[k,obs[i+1]]*beta[i+1,k])/prob_o_lambda\n",
    "                    #x[i,j,k] = alpha(i,j) + self.A[j,k] + self.B[k,obs[i+1]] + beta(i+1,k) - logprob\n",
    "                    xi[i,j,k] = alpha[i,j] + log_A[j,k] + log_B[k,obs[i+1]] + beta[i+1,k] - logprob\n",
    "        \n",
    "        gamma = alpha + beta - logprob\n",
    "            \n",
    "        return logprob, xi, gamma\n",
    "\n",
    "    def learn_unsupervised(self, corpus, num_iters):\n",
    "        \"\"\"Run the Baum Welch EM algorithm\n",
    "        \"\"\"\n",
    "        print(len(corpus))\n",
    "        temp_list = []\n",
    "        for i_iter in range(num_iters):\n",
    "            print(i_iter)\n",
    "            expected_si = np.zeros(self.num_states) # your code here\n",
    "            expected_sj = np.zeros(self.num_states)\n",
    "            expected_sij = np.zeros((self.num_states, self.num_states)) # your code here\n",
    "            expected_sjwk = np.zeros((self.num_states, self.num_words))\n",
    "            total_logprob = 0\n",
    "            \n",
    "            #print('A')\n",
    "            #print(self.A)\n",
    "            #print('B')\n",
    "            #print(self.B)\n",
    "            #print(np.sum(self.B, axis = 1))\n",
    "            #print('pi')\n",
    "            #print(self.pi)\n",
    "            \n",
    "            pi_new = np.zeros(self.num_states)         \n",
    "            num_review = 0\n",
    "            \n",
    "            for review in corpus:\n",
    "                #print(str(num_review))\n",
    "                num_review += 1\n",
    "                logprob, xi, gamma = self.forward_backward(review)\n",
    "                # your code here\n",
    "                exp_xi = np.exp(xi)\n",
    "                exp_gamma = np.exp(gamma)\n",
    "                #print(exp_gamma)\n",
    "                #total_logprob = np.logaddexp(total_logprob, logprob)\n",
    "                total_logprob += logprob\n",
    "                expected_si = expected_si + np.sum(exp_gamma[:-1,:], axis = 0)\n",
    "                expected_sj = expected_sj + np.sum(exp_gamma, axis = 0)\n",
    "                expected_sij = expected_sij + np.sum(exp_xi[:-1,:,:], axis = 0)\n",
    "                \n",
    "                #expected_sjwk_review = np.zeros((self.num_states, self.num_words))\n",
    "                for idx in range(len(review)):\n",
    "                    #for s in range(self.num_states):\n",
    "                    expected_sjwk[:,review[idx]] = expected_sjwk[:,review[idx]] + exp_gamma[idx,:].T\n",
    "                #for j in range(self.num_states):\n",
    "                    #expected_si[j] = np.logaddexp(expected_si[j], logsumexp(gamma[:-1,j]))\n",
    "                    #expected_sj[j] = np.logaddexp(expected_sj[j], logsumexp(gamma[:,j]))\n",
    "                    #for k in range(self.num_states):\n",
    "                        #expected_sij[j,k] = np.logaddexp(expected_sij[j,k], logsumexp(xi[:-1,j,k]))\n",
    "                                       \n",
    "                #for i in range(len(review)):\n",
    "                    #expected_sjwk[:,review[i]] = np.logaddexp(expected_sjwk[:,review[i]], np.transpose(gamma[i,:]))\n",
    "                \n",
    "                pi_new += exp_gamma[0,:]\n",
    "            \n",
    "            temp_list.append(total_logprob)\n",
    "            print(\"log-likelihood\", total_logprob)\n",
    "            \n",
    "            if i_iter > 0:\n",
    "                print(abs(temp_list[i_iter]/temp_list[i_iter - 1] - 1))\n",
    "                if abs(temp_list[i_iter]/temp_list[i_iter - 1] - 1) < 1e-4:\n",
    "                    break\n",
    "                \n",
    "            A_new = np.zeros((self.num_states, self.num_states)) # your code here\n",
    "            B_new = np.zeros((self.num_states, self.num_words)) # your code here\n",
    "            pi_new = pi_new/len(corpus)\n",
    "            \n",
    "            for i in range(self.num_states):\n",
    "                A_new[i,:] = expected_sij[i,:]/expected_si[i]\n",
    "                B_new[i,:] = expected_sjwk[i,:]/expected_sj[i]\n",
    "                    #A_new[i,j] = expected_sij[i,j] - expected_si[i]\n",
    "                                                                              \n",
    "            self.A = A_new\n",
    "            self.B = B_new\n",
    "            self.pi = pi_new "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eF-l7WucpCBP"
   },
   "source": [
    "Train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTWXUt15pDg4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "3000\n",
      "0\n",
      "log-likelihood -2101804.921020325\n",
      "1\n",
      "log-likelihood -1524236.8993465118\n",
      "0.2747962077248499\n",
      "2\n",
      "log-likelihood -1521976.7071049106\n",
      "0.0014828352748645912\n"
     ]
    }
   ],
   "source": [
    "tokenizer = lab_util.Tokenizer()\n",
    "tokenizer.fit(train_reviews)\n",
    "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
    "log_alpha = hmm.forward(train_reviews_tk[1])\n",
    "#print('alpha')\n",
    "#print(log_alpha)\n",
    "log_beta = hmm.backward(train_reviews_tk[1])\n",
    "#print('beta')\n",
    "#print(log_beta)\n",
    "#log_prob, xi, gamma = hmm.forward_backward(train_reviews_tk[2])\n",
    "#print('gamma')\n",
    "#print(log_alpha + log_beta - log_prob)\n",
    "#print('log_prob')\n",
    "#print(log_prob)\n",
    "#print(xi.shape)\n",
    "hmm.learn_unsupervised(train_reviews_tk, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiCwE05xqXmI"
   },
   "source": [
    "Let's look at some of the words associated with each hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXhMoLUFqbn_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0\n",
      "www 8.644481953986294e-45\n",
      "http 1.7180692889504047e-42\n",
      "suppose 9.680339437106044e-36\n",
      "marzano 2.9985202824174896e-34\n",
      "gp 2.1749795483487064e-31\n",
      "tend 1.4391562299843026e-29\n",
      "able 2.336634017041316e-29\n",
      "moved 8.378941790468853e-29\n",
      "hoped 1.0265405340036787e-27\n",
      "decided 9.409025624544188e-27\n",
      "\n",
      "state 1\n",
      "marzano 1.7719159471921054e-52\n",
      "www 4.4854344888031915e-41\n",
      "gp 7.441701195224051e-37\n",
      "http 1.554690438322806e-33\n",
      "suppose 2.8824713807736332e-27\n",
      "am 8.583263494025701e-25\n",
      "son's 1.8480420691818067e-24\n",
      "splash 1.3228306660557773e-22\n",
      "figured 3.076055841538382e-22\n",
      "spent 6.104758881781126e-22\n",
      "\n",
      "state 2\n",
      "gp 1.4497052198202335e-31\n",
      "0g 1.843371532450621e-26\n",
      "www 2.205823505639042e-25\n",
      "concentrates 1.3924720263560608e-21\n",
      "agave 1.695486912144952e-21\n",
      "http 3.272955807604809e-21\n",
      "puck 1.0589084397216047e-20\n",
      "splash 6.898083121263165e-20\n",
      "nectar 8.1374679796819e-20\n",
      "dijon 1.982865450320652e-18\n",
      "\n",
      "state 3\n",
      "gp 3.708780741216216e-45\n",
      "www 2.670569709561586e-35\n",
      "http 3.734085286686433e-32\n",
      "grey 7.16115913799756e-23\n",
      "son's 7.144975664382387e-20\n",
      "agave 8.486167538613439e-20\n",
      "marzano 3.044787889775734e-19\n",
      "hadn't 6.058827309104322e-19\n",
      "science 1.5877140675652795e-18\n",
      "puck 1.8205595801126832e-18\n",
      "\n",
      "state 4\n",
      "http 6.410844338787752e-48\n",
      "earl 2.753258213626215e-44\n",
      "0g 2.2408535890887767e-40\n",
      "marzano 2.393594358709681e-39\n",
      "wolfgang 2.9908857749725177e-38\n",
      "gp 5.600352392367203e-37\n",
      "science 3.46609674138211e-24\n",
      "hint 5.00346093715358e-23\n",
      "splash 5.417473064844402e-23\n",
      "bunch 9.036229843668478e-23\n",
      "\n",
      "state 5\n",
      "http 1.942101365052381e-52\n",
      "marzano 1.6451685066848701e-44\n",
      "www 7.268408879136656e-41\n",
      "gp 1.0358515214195441e-34\n",
      "earl 2.885310348406155e-29\n",
      "hadn't 4.5108328091546146e-29\n",
      "wolfgang 1.258752080339496e-26\n",
      "grey 2.748428611063603e-26\n",
      "0g 9.395233437633858e-24\n",
      "wife 1.8752227764644193e-22\n",
      "\n",
      "state 6\n",
      "http 1.3344300740300409e-33\n",
      "www 2.438530845068081e-30\n",
      "marzano 1.8615541626177143e-27\n",
      "earl 6.600179795430816e-27\n",
      "nectar 1.5018052618788485e-23\n",
      "0g 2.7192290726906948e-22\n",
      "whenever 1.477659656412383e-21\n",
      "am 6.843593584966045e-21\n",
      "drank 4.6747496294216744e-20\n",
      "husband 6.581147061485645e-20\n",
      "\n",
      "state 7\n",
      "gp 1.4772829121533373e-38\n",
      "nectar 5.550877796261851e-30\n",
      "http 6.909500511851844e-23\n",
      "costa 5.627413472915515e-19\n",
      "hint 7.42774759562865e-19\n",
      "agave 1.042840667769361e-18\n",
      "hadn't 3.0041783840484997e-18\n",
      "href 3.1613113109970126e-18\n",
      "harmony 3.625408336898156e-18\n",
      "splash 9.228710224640893e-18\n",
      "\n",
      "state 8\n",
      "gp 4.8747819899605095e-40\n",
      "www 3.8197136745145765e-34\n",
      "concentrates 1.4936368188423832e-27\n",
      "agave 2.9005325632931788e-24\n",
      "mg 3.0494850432064886e-24\n",
      "nectar 1.981830104334688e-23\n",
      "bunch 5.494490694073331e-23\n",
      "earl 5.861886868073151e-23\n",
      "wolfgang 1.0475087369158449e-22\n",
      "alive 1.295381554273969e-22\n",
      "\n",
      "state 9\n",
      "http 2.0822442641457377e-57\n",
      "www 3.3250718020490218e-34\n",
      "hadn't 1.4060989798196125e-29\n",
      "marzano 1.7419727931320818e-26\n",
      "gp 4.7273400548252536e-26\n",
      "wife 9.925836590499847e-26\n",
      "costa 1.524914670055648e-23\n",
      "pleasantly 7.462641558903267e-22\n",
      "watered 5.160217209080439e-21\n",
      "husband 5.91523743583938e-21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hmm.num_states):\n",
    "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
    "    print(f\"state {i}\")\n",
    "    for o in most_probable:\n",
    "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAQ_PmASwdFz"
   },
   "source": [
    "We can also look at some samples from the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tj1eT3s3wgFJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i <unk> control no <unk> seem that actual delicious on']\n",
      "['no <unk> sodas orange food these any best from it']\n",
      "['one . <unk> coconut . <unk> tea organic hot of']\n",
      "['itself the amazon because so <unk> us im started the']\n",
      "['brands , . the the br no br amazon <unk>']\n",
      "[\"hoping <unk> value . , . but i'm on fine\"]\n",
      "['as think decided wanted delicious was with many or really']\n",
      "['great gum store such savory bit problems added for the']\n",
      "['it taste ! used the <unk> br <unk> iron ,']\n",
      "['diet cup in up <unk> on find cooked if and']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9Qk9adNr7lQ"
   },
   "source": [
    "Finally, let's repeat the classification experiment from Parts 1 and 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
    "\n",
    "(Warning! results may not be the same as in earlier versions of this experiment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mL6JQXLJspyA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm features, 3000 examples\n",
      "test accuracy 0.618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingye/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def train_model(xs_featurized, ys):\n",
    "  import sklearn.linear_model\n",
    "  model = sklearn.linear_model.LogisticRegression()\n",
    "  model.fit(xs_featurized, ys)\n",
    "  return model\n",
    "\n",
    "def eval_model(model, xs_featurized, ys):\n",
    "  pred_ys = model.predict(xs_featurized)\n",
    "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
    "\n",
    "def training_experiment(name, featurizer, n_train):\n",
    "    print(f\"{name} features, {n_train} examples\")\n",
    "    train_xs = np.array([\n",
    "        hmm_featurizer(tokenizer.tokenize(review)) \n",
    "        for review in train_reviews[:n_train]\n",
    "    ])\n",
    "    train_ys = train_labels[:n_train]\n",
    "    test_xs = np.array([\n",
    "        hmm_featurizer(tokenizer.tokenize(review))\n",
    "        for review in test_reviews\n",
    "    ])\n",
    "    test_ys = test_labels\n",
    "    model = train_model(train_xs, train_ys)\n",
    "    eval_model(model, test_xs, test_ys)\n",
    "    print()\n",
    "\n",
    "def hmm_featurizer(review):\n",
    "    flat_review = []\n",
    "    for i in review:\n",
    "        #print(i)\n",
    "        if i != []:\n",
    "            flat_review.append(i[0])\n",
    "            \n",
    "    _, _, gamma = hmm.forward_backward(flat_review)\n",
    "    return gamma.sum(axis=0)\n",
    "\n",
    "training_experiment(\"hmm\", hmm_featurizer, n_train=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9W0qx41Bu2aF"
   },
   "source": [
    "**Part 3: Lab writeup**\n",
    "\n",
    "1. What do the learned hidden states seem to encode when you run unsupervised \n",
    "   HMM training with only 2 states? What about 10? What about 100?\n",
    "\n",
    "2. As before, what's the relationship between # of labeled examples and    \n",
    "   usefulness of HMM-based sentence representations? Are these results generally\n",
    "   better or worse than in Parts 1 and 2 of the homework? Why or why not might\n",
    "   HMM state distributions be sensible sentence representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZu3XLyDSbfpOdgh0IV6ip",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "6864_hw1b",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
